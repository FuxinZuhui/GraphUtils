# PyG的图数据
## Data
一个单独的图在PyG中表示为`torch_geometric.Data`,`Data`类有以下属性:

- `data.x`: 节点特征矩阵——`[num_nodes, num_node_features]`
- `data.edge_index`:COO格式的图连通关系`[2, num_edges]`类型是`torch.long`
- `data.edge_attr`: 边特征矩阵——`[num_edges, num_edge_features]`
- `data.y`: 训练标签, 节点级标签——`[num_nodes, *]` or 图级别标签——`[1, *]`
- `data.pos`: 节点位置矩阵(理解成地理位置坐标)`[num_nodes, num_dimensions]`

这些attr都不是必需的。Data对象不局限于这些属性。例如，扩展data.face保存3D网格中的连通性，该张量的形状为`[3,num_faces]`，类型为`torch.long`。

`<simple example>`

```python
import torch
from torch_geometric.data import Data
# 注意这里的无向边是用双向边表示的
edge_index = torch.tensor([[0, 1, 1, 2],
                           [1, 0, 2, 1]], dtype=torch.long)
x = torch.tensor([[-1], [0], [1]], dtype=torch.float)

data = Data(x=x, edge_index=edge_index)
print(data)
# Data(x=[3, 1], edge_index=[2, 4])
```
<center><img src=" .\pics\1.1.example_1.png " width="500" height = "250" ></center>

### Data的方法

```python
print(data.keys)
print("-" * 50)
print(data['x'])
print("-" * 50)
for key, item in data:
    print(f'{key} found in data')
print("-" * 50)
print('edge_attr' in data)
print("-" * 50)
print(data.num_nodes)
print("-" * 50)
print(data.num_edges)
print("-" * 50)
print(data.num_node_features)
print("-" * 50)
print(data.has_isolated_nodes())
print("-" * 50)
print(data.has_self_loops())
print("-" * 50)
print(data.is_directed())
print("-" * 50)

# Transfer data object to GPU.
device = torch.device('cuda')
data = data.to(device)
```

## 通用基准数据集
PyG 包含大量通用基准数据集，例如，所有 Planetoid 数据集（Cora、Citeseer、Pubmed）、来自[http://graphkernels.cs.tu-dortmund.de](http://graphkernels.cs.tu-dortmund.de)的所有图形分类数据集及其cleaned版本、QM7 和 QM9数据集，以及少数 3D 网格/点云数据集，如 FAUST、ModelNet10/40 和 ShapeNet。

初始化数据集很简单。数据集的初始化将自动下载其原始文件并将其处理为先前描述的Data格式。 例如，要加载 ENZYMES 数据集(由 6 个类中的 600 个图组成):
```python
from torch_geometric.datasets import TUDataset

dataset = TUDataset(root='../datas/ENZYMES', name='ENZYMES')
print("length of ENZYMES:{}".format(len(dataset)))
print("num classes:{}".format(dataset.num_classes))
print("num node features:{}".format(dataset.num_node_features))
```

## mini-batch
PyG 通过创建稀疏块对角邻接矩阵(由edge_index定义)并在**节点维度连接特征和目标矩阵**来实现mini-batch的并行化。这种组合允许在一批示例中使用不同数量的节点和边。

- 实际上是先将矩阵分块，然后根据分块**重排节点**(包括特征及其对应的label)

$$
\begin{split}\mathbf{A} = \begin{bmatrix} \mathbf{A}_1 & & \\ & \ddots & \\ & & \mathbf{A}_n \end{bmatrix}, \qquad \mathbf{X} = \begin{bmatrix} \mathbf{X}_1 \\ \vdots \\ \mathbf{X}_n \end{bmatrix}, \qquad \mathbf{Y} = \begin{bmatrix} \mathbf{Y}_1 \\ \vdots \\ \mathbf{Y}_n \end{bmatrix}\end{split}
$$

PyG的`torch_geometric.loader.DataLoader`，自动处理该连接过程。

### 关于mini-batch的进一步解释

- [torch_geometric.data.Batch](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Batch)

首先解释PyG中的Data类:
1. Data类的x是`shape: [num_nodes, num_node_features]`
2. Data类的edge_index是`shape: [2, num_edges]`,其中2这个维度是起始出节点索引和入节点索引,而该索引是与x中的`num_nodes`一一对应的。
   
对于多图数据(每个样本一个图,比如`ENZYMES`), 实际上mini-batch在PyG中的实现是将采样到的样本的节点全部concat在一起,然后reindex,其中包括edge_index的重索引以及节点的重索引。

举例来说,假设`batch_size = 3`, 假设采样到第`[1,2,3]`的样本,那么第二个采样到的样本的`node_index`就是在第一个样本后继续排。

其中有:
```python
import torch
x1 = torch.tensor([[1],[1.1],[0.1]], dtype = torch.float)
x2 = torch.tensor([[2],[2.2],[0.2]], dtype = torch.float)
x3 = torch.tensor([[3],[3.3],[0.3]], dtype = torch.float)

# 公有的edge_index
global_edge_index = torch.tensor([[0, 1, 1, 2],
                                  [1, 0, 2, 1]], dtype=torch.long)
```
那么最后得到的batch如下:
```python
print(batch.x)
# tensor([[1.0],[1.1],[0.1],
#         [2.0],[2.2],[0.2],
#         [3.0],[3.3],[0.3]])

print(batch.edge_index)
# tensor([0, 1, 1, 2, 3, 4, 4, 5, 6, 7, 7, 8],
#        [1, 0, 2, 1, 4, 3, 5, 4, 7, 6, 8, 7])

print(batch.batch) 
# tensor([0, 0, 0, 1, 1, 1, 2, 2, 2])

# 对应batch.x的切分段: len(batch.batch) + 1
print(batch.ptr) 
# tensor([0,3,6,9])
```
## Batch子图处理

`torch_geometric.data.Batch.batch`是一个列向量，它将每个节点映射到批处理中的相应graph中：

$$
\mathrm{batch} = {\begin{bmatrix} 0 & \cdots & 0 & 1 & \cdots & n - 2 & n -1 & \cdots & n - 1 \end{bmatrix}}^{\top}
$$

可以使用它来为每个图单独计算节点维度中的平均节点特征：

```python
from torch_scatter import scatter_mean

dataset = TUDataset(root= DATA_PATH + 'ENZYMES', name='ENZYMES', use_node_attr=True)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

for data in loader:
    print(data)
    print(data.num_graphs)
    x = scatter_mean(data.x, data.batch, dim=0)
    print(x.size())
    break
```

## 数据转换
PyG自带转换方法, 接受一个`Data`对象作为输入并返回一个新的`Data`对象。**转换**可以使用`torch_geometric.transforms.Compose`链接在一起，并在**将处理过的数据集保存到磁盘**之前(`pre_transform`)或访问数据集中的Graph之前(`transform`)应用。

```python
import torch_geometric.transforms as T
from torch_geometric.datasets import ShapeNet

dataset = ShapeNet(root=DATA_PATH + 'ShapeNet', categories=['Airplane'],
                    pre_transform=T.KNNGraph(k=6),
                    transform=T.RandomTranslate(0.01))
# lastest version: RandomTranslate convert to RandomJitter

print(dataset[0])
print(dataset[0].edge_index)
print(dataset[0].y)
```
PyG使用`pre_transform`来在**将数据保存到磁盘之前对其进行转换**(从而加快加载时间)。

下一次初始化数据集时，它已经包含图Graph Edges，即使没有通过任何转换。如果`pre_transform`与已处理数据集中的不匹配，将收到警告。

## PyG构造图神经网络

```python
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.datasets import Planetoid


dataset = Planetoid(root= DATA_PATH + 'Cora', name='Cora')
print(dataset[0])

class GCN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # GCNConv类的forward类首先调用基类MessagePassing中的propogate,
        # propogate会调起覆写的message_and_passing()
        self.conv1 = GCNConv(dataset.num_node_features, 16)
        self.conv2 = GCNConv(16, dataset.num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)

        return F.log_softmax(x, dim=1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
gcn = GCN().to(device)
data = dataset[0].to(device)
optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01, weight_decay=5e-4)

gcn.train()
for epoch in range(100):
    optimizer.zero_grad()
    out = gcn(data)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

gcn.eval()
pred = gcn(data).argmax(dim=1)
correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()
acc = int(correct) / int(data.test_mask.sum())
print(f'Accuracy: {acc:.4f}')
```
构造函数定义了两个`GCNConv`层，在网络的前向传递中被调用。
- 请注意，**非线性没有集成在conv**中，因此需要在之后应用(**这在 PyG 中的所有运算符中都是一致的**)。