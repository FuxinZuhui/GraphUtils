# MessagePassing NN

将卷积算子推广到拓扑结构上通常表示为**邻域聚合**或**消息传递**的范式。

- $\mathbf{x}^{(k-1)}_i \in \mathbb{R}^F$
表示节点$i$在$(k-1)$层的节点特征
- $\mathbf{e}_{j,i} \in \mathbb{R}^D$表示(可选)节点$i$到节点$j$的边特征。
- 消息传递图神经网络可以描述为:
  $$\mathbf{x}_i^{(k)} = \gamma^{(k)} \left( \mathbf{x}_i^{(k-1)}, \square_{j \in \mathcal{N}(i)} \, \phi^{(k)}\left(\mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)},\mathbf{e}_{j,i}\right) \right)$$

$\square$ 表示**可微的置换不变函数**(即输入变量顺序不影响结果)，例如sum、mean 或 max，$\gamma$ 和 $\phi$ 表示可微分函数，例如 MLP(多层感知器)。

# MessagePassing基类
PyG 提供了MessagePassing基类，它通过自动处理消息传播来创建消息传递图神经网络。

用户定义功能:
1. $\phi$,即`message()`
2. $\gamma$,即`update()`
3. 使用的aggregate方法，即`aggr="add"`，`aggr="mean"`或`aggr="max"`。


- **`MessagePassing(aggr ="add", flow="source_to_target",node_dim=-2)`**
  - `aggr`:聚合方案,`("add", "mean" or "max")`
  - `flow`:信息传递方向,`("source_to_target","target_to_source")`
  - `node_dim`: 聚合哪个维度, `-2`在这里指的是`[num_nodes, num_node_features]`的`-2`

- **`MessagePassing.propagate(edge_index, size=None, **kwargs)`**   
    调起信息传递的函数  
    如果默认`size=None`,那么会生成一个`[N,N]`方阵。   
    不仅仅可以用在传播方阵`[N,N]`,也可以定义一个二部图(bipartite graphs)`[N,M]`,此时size参数需要传入。这种方法下`x=(x_N,x_M)`

- **`MessagePassing.message(...)`**
  - 构建传递给节点$i$的消息,即$\phi$
  1) 如果flow="source_to_target"，则是$(j,i)\in\mathcal{E}$的边的集合；(此时$i$是target,所以是$j\rightarrow i$的边)
  2) 如果flow="target_to_source"，则是$(i,j)\in\mathcal{E}$的边的集合。

  - 可以获取传递给`propagate()`的任何参数
  
  - 除此之外, 传递给`propagate()`的tensor会被拆分为属于中心节点($i$)和邻接节点($j$)的部分, 只需要在变量名后加`_i`和`_j`。
  1) 如果是节点的features(即$x$)，定义的`meassage`方法包含参数`x_i`，那么首先propagate()方法会将节点表征拆分成中心节点表征`x_i`和邻接节点表征`x_j`,并会传递给`message`。

- **`MessagePassing.update(aggr_out, ...)`** 

    更新关于节点的节点表征,对于每个节点$i$采用$\gamma$的逻辑。
  - `aggr_out`:第一个参数是`aggregate()`的输出
  - `kwargs`:传递给`propagate()`的任何参数

# GCN Implements
$$\mathbf{x}_i^{(k)} = \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta}^{\top} \cdot \mathbf{x}_j^{(k-1)} \right),$$

其中**相邻节点特征**首先由权重矩阵$\Theta$的度归一化。这个公式可以分为以下几个步骤：

1. 将self-loop添加到邻接矩阵。
2. 线性变换节点特征矩阵。
3. 计算归一化系数。
4. 归一化节点特征.
5. 聚合相邻节点特征("add" aggregate)。

```python
import torch
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree

class GCN(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super().__init__(aggr="add")
        self.lin = torch.nn.Linear(in_channels, out_channels)
    def forward(self, x, edge_index):
        '''
        x: [num_nodes, num_node_features]
        edge_index: [2, num_edges]
        '''
        # 添加自回环
        edge_index, _ = add_self_loops(edge_index, num_nodes = x.size(0))
        #x = self.lin(x)

        row, col = edge_index
        # TODO:pyG官方文档degree计算包括self-loop,是否包括?
        deg = degree(col, x.size(0))
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        # feature归一化需要除以deg(i)**(-0.5)和deg(j)**(-0.5)
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]
        
        return self.propagate(edge_index, x=x, norm=norm)

    def message(self, x_j, norm):
        '''
        x_j: [num_edges, out_channels],代表利用每个edge_index关联到的node
        norm: [num_edges]
        '''
        print(norm)
        return norm.view(-1,1) * x_j
```
1. degree计算:
- 由于无向图的edge是以双向边代替无向边,因此degree只需要算`edge_index`其中一个维度就可以。
- REF: [torch_geometric.utils.degree](https://pytorch-geometric.readthedocs.io/en/2.0.3/modules/utils.html#torch_geometric.utils.degree)

# EdgeConv Implements

边卷积层定义为

$$\mathbf{x}_i^{(k)} = \max_{j \in \mathcal{N}(i)} h_{\mathbf{\Theta}} \left( \mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)} - \mathbf{x}_i^{(k-1)} \right),$$

$h_{\mathbf{\Theta}}$表示 MLP。使用"max"的MessagePassing类来实现这一层

```python
from torch.nn import Sequential as Seq, Linear, ReLU

class EdgeConv(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super().__init__(aggr = "max")
        self.mlp = Sequential(
            Linear(2 * in_channels, out_channels),
            Relu(),
            Linear(out_channels, out_channels)
        )


```