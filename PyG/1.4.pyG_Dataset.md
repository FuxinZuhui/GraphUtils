# pyG Dataset
> **抽象类**          

pyG为数据集提供了两个抽象类：
1. `torch_geometric.data.Dataset`
2. `torch_geometric.data.InMemoryDataset`. 对于占用内存有限的数据集，可以将整个数据集的数据都存储到内存/CPU里。PyG为提供了构造数据完全存于内存的数据集类,即`InMemoryDataset`类。

每个数据集都会传递一个**根文件夹**，该根文件夹指示**数据集的存储位置**。我们将根文件夹分成两个文件夹：
1. `raw_dir`保存下载到的数据集
2. `processed_dir`保存处理后的数据集。

> **默认参数**    

每个数据集可以传递默认参数(`default: None`): 
1. `transform`:**访问**之前动态转换数据对象(因此最好用于数据扩充)
2. `pre_transform`:将数据对象**保存到磁盘之前**应用转换(因此它最好用于只需要执行一次的**大量预计算**)
3. `pre_filter`: 在**保存前**过滤掉数据对象, 可能涉及**限制数据对象属于特定类**的功能。

# Creating “In Memory Datasets”

创建`torch_geometric.data.InMemoryDataset`需要实现四个方法:

1. `torch_geometric.data.InMemoryDataset.raw_file_names()`:list类型，包含保存在raw_dir内部的文件——为了跳过下载
2. `torch_geometric.data.InMemoryDataset.processed_file_names()`: list类型，包含保存在processed_dir内部的文件——为了跳过数据处理
3. `torch_geometric.data.InMemoryDataset.download()`: 下载原始数据到raw_dir
4. `torch_geometric.data.InMemoryDataset.process()`: 处理原始数据保存到processed_dir

> `torch_geometric.data`预设了一部分保存和提取数据的方法

The real magic happens in the body of process(). Here, we need to read and create a list of Data objects and save it into the processed_dir. Because saving a huge python list is rather slow, we collate the list into one huge Data object via torch_geometric.data.InMemoryDataset.collate() before saving . The collated data object has concatenated all examples into one big data object and, in addition, returns a slices dictionary to reconstruct single examples from this object. Finally, we need to load these two objects in the constructor into the properties self.data and self.slices.

`process()`函数读取一个Data类型为元素的列表, 并且保存到processed_dir。保存一个巨大的python列表速度十分缓慢, 因此利用`torch_geometric.data.InMemoryDataset.collate()`在保存之前该列表整理(collate)进一个Data对象当中。

collate结束的data对象已经是所有的样本连接成一个巨大的data对象。除此之外, 返回一个切片字典来从对象中重新构造单个字典。

最后, 需要从构造器中加载两个对象成self.data和self.slices属性。